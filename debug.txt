Of course. Here is the complete, newly enhanced code for the summary and chat endpoints. These are the "analysis" features that build on top of your completed transcripts.

The code is designed to be state-aware, persistent, and efficient. It correctly checks if the required data (like a diarized transcript) exists before running, and it saves the results to the database to avoid costly re-generation.

Full Code for Summary and Chat Endpoints

This code should be placed in the second half of your app/api/routes/meeting.py file, replacing the previous placeholders.

code
Python
download
content_copy
expand_less

# in app/api/routes/meeting.py

# --- Add these to your imports at the top of the file ---
from app.schemas.meeting import (
    SummaryRequest, SummaryResponse, ChatRequest, ChatResponse
)
from app.api.deps import get_job_with_any_transcript, get_job_with_completed_diarization
from app.services.ai_service import ai_service
from app.db.models import Summary, ChatHistory

# ===================================================================
#   Analysis, Chat, and Download Endpoints
# ===================================================================

@router.post(
    "/{request_id}/summary",
    response_model=SummaryResponse,
    summary="Generate a meeting summary"
)
async def generate_summary(
    summary_request: SummaryRequest,
    db: Session = Depends(get_db_session),
    job: MeetingJob = Depends(get_owned_job_from_path) # Base ownership check
):
    """
    Generates a summary for the meeting based on the requested type.
    - 'topic', 'action_items', 'decision_log' require a completed transcript.
    - 'speaker' requires a completed, speaker-separated transcript.

    If a summary of the same type already exists, it will be overwritten.
    """
    summary_type = summary_request.summary_type
    logger.info(f"Received request to generate '{summary_type}' summary for job '{job.request_id}'.")

    # --- State Validation ---
    if summary_type == "speaker":
        # Stricter check for speaker-based summaries
        if not job.diarized_transcript:
            raise HTTPException(
                status_code=status.HTTP_400_BAD_REQUEST,
                detail="A 'speaker' summary requires a completed speaker-separated transcript. Please run diarization first."
            )
        # Use the richer diarized transcript as the source
        transcript_source = job.diarized_transcript.transcript_data
        # Format for LLM: "Speaker Name: text..."
        source_text = "\n".join([f"{seg['speaker']}: {seg['text']}" for seg in transcript_source])
    else:
        # Standard check for all other summary types
        transcription_entry = db.exec(
            select(Transcription).where(
                Transcription.meeting_job_id == job.id,
                Transcription.language == job.language
            )
        ).first()
        if not transcription_entry or not transcription_entry.transcript_data:
            raise HTTPException(
                status_code=status.HTTP_400_BAD_REQUEST,
                detail="A summary requires a completed transcript. Please ensure transcription is complete."
            )
        # Use the plain transcript as the source
        transcript_source = transcription_entry.transcript_data
        source_text = "\n".join([seg['text'] for seg in transcript_source])

    # --- LLM Call ---
    try:
        meeting_info = {
            "bbh_name": job.bbh_name,
            "meeting_type": job.meeting_type,
            "meeting_host": job.meeting_host,
        }
        summary_content = await ai_service.get_response(
            task=summary_type,
            user_message=source_text,
            context={"meeting_info": meeting_info}
        )
    except Exception as e:
        logger.error(f"AI service failed during summary generation for job '{job.request_id}': {e}", exc_info=True)
        raise HTTPException(status_code=502, detail=f"Failed to get response from AI service: {e}")

    # --- Persist the Result (Update or Create) ---
    existing_summary = db.exec(
        select(Summary).where(
            Summary.meeting_job_id == job.id,
            Summary.summary_type == summary_type
        )
    ).first()

    if existing_summary:
        logger.info(f"Updating existing '{summary_type}' summary for job '{job.request_id}'.")
        existing_summary.summary_content = summary_content
        summary_to_return = existing_summary
    else:
        logger.info(f"Creating new '{summary_type}' summary for job '{job.request_id}'.")
        new_summary = Summary(
            meeting_job_id=job.id,
            summary_type=summary_type,
            summary_content=summary_content
        )
        db.add(new_summary)
        summary_to_return = new_summary
    
    db.commit()
    db.refresh(summary_to_return)

    return SummaryResponse(
        request_id=job.request_id,
        summary_type=summary_to_return.summary_type,
        summary_content=summary_to_return.summary_content
    )


@router.post(
    "/chat",
    response_model=ChatResponse,
    summary="Chat about the meeting content"
)
async def chat_with_meeting(
    chat_request: ChatRequest,
    db: Session = Depends(get_db_session)
    # Note: We manually fetch the job here because the request_id is in the body
):
    """
    Handles conversational queries about a completed meeting. It uses the
    transcript, summaries, and recent conversation history as context.
    """
    job = db.exec(select(MeetingJob).where(MeetingJob.request_id == chat_request.requestId)).first()
    if not job:
        raise HTTPException(status_code=404, detail="Meeting job not found.")
    
    # --- Gather Context for the LLM ---
    # 1. Get Transcript
    transcription_entry = db.exec(
        select(Transcription).where(
            Transcription.meeting_job_id == job.id,
            Transcription.language == job.language
        )
    ).first()
    if not transcription_entry:
        raise HTTPException(status_code=400, detail="Cannot chat without a completed transcript.")
    transcript_text = "\n".join([seg['text'] for seg in transcription_entry.transcript_data])

    # 2. Get Summaries
    summaries = db.exec(select(Summary).where(Summary.meeting_job_id == job.id)).all()
    summary_texts = [f"--- SUMMARY ({s.summary_type.upper()}) ---\n{s.summary_content}" for s in summaries]

    # 3. Get Chat History
    chat_history_db = db.exec(
        select(ChatHistory)
        .where(ChatHistory.meeting_job_id == job.id)
        .order_by(ChatHistory.created_at.desc())
        .limit(settings.LIMIT_TURN * 2) # Fetch user and assistant messages
    ).all()
    # Reverse to get chronological order
    chat_history_formatted = [{"role": entry.role, "content": entry.message} for entry in reversed(chat_history_db)]

    # --- Format the final prompt ---
    full_context_for_llm = (
        f"--- MEETING TRANSCRIPT ---\n{transcript_text}\n\n" +
        "\n\n".join(summary_texts)
    )

    # --- LLM Call ---
    try:
        assistant_response = await ai_service.get_response(
            task="chat",
            user_message=f"**User Question:**\n{chat_request.message}\n\n**Meeting Context:**\n{full_context_for_llm}",
            context={"history": chat_history_formatted}
        )
    except Exception as e:
        logger.error(f"AI service failed during chat for job '{job.request_id}': {e}", exc_info=True)
        raise HTTPException(status_code=502, detail=f"Failed to get response from AI service: {e}")

    # --- Persist Conversation ---
    user_message_entry = ChatHistory(
        meeting_job_id=job.id,
        role="user",
        message=chat_request.message
    )
    assistant_message_entry = ChatHistory(
        meeting_job_id=job.id,
        role="assistant",
        message=assistant_response
    )
    db.add(user_message_entry)
    db.add(assistant_message_entry)
    db.commit()

    return ChatResponse(response=assistant_response)

# ... (rest of the meeting.py file, like the download endpoint)
Files That Need to Be Correct (And How We've Accommodated Them)

To support these two endpoints, several other files must be correctly set up. Based on our previous steps, they should already be in the correct state. This serves as a final checklist.

app/db/models.py:

Requirement: Must have the Summary and ChatHistory tables defined.

Requirement: The MeetingJob model must have the summaries: List["Summary"] and chat_history: List["ChatHistory"] relationships defined with cascade="all, delete".

Status: We have already defined these models correctly in a previous step.

app/schemas/meeting.py:

Requirement: Must have the Pydantic schemas: SummaryRequest, SummaryResponse, ChatRequest, and ChatResponse.

Status: We have already defined these schemas correctly.

app/services/ai_service.py:

Requirement: Must have the system prompts for "chat" and all the summary types ("topic", "speaker", etc.) defined in the _get_system_prompt_for_task method's dictionary.

Status: We have already defined these prompts.

app/core/config.py:

Requirement: Must have the LIMIT_TURN setting for the chat history.

Status: We have already added this setting.

app/api/deps.py:

Requirement: Must have the dependencies for verifying job ownership and state, like get_owned_job_from_path.

Status: We have already defined these dependencies.

By implementing the code above, your meeting.py router is now fully equipped with powerful, state-aware, and persistent summary and chat functionalities that seamlessly integrate with the rest of your application's architecture.
