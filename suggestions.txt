Of course. This is an excellent set of requirements that will significantly enhance the application's capabilities. You want to unify the summary system so that all summary types are persistent, editable via chat, and downloadable as documents.

Here is a comprehensive guide on how to modify your code to achieve this, incorporating your coworker's document generation logic for the standard summaries.

Answering Your Initial Questions

Can bbh_hdqt and nghi_quyet be added to the chat editing options?
Yes, absolutely. We will teach the "Intent Analysis" AI to recognize them as valid summary types (entities).

Does the present code store the summary for nghi_quyet and bbh_hdqt?
No, it does not. Your current generate_and_download_document endpoint generates them on-the-fly for a one-time download and never saves them to the database. We will change this behavior to make them persistent.

How can we handle downloading all summary types?
We will refactor the download endpoint into a single, intelligent function. It will fetch the requested summary from the database. If it's bbh_hdqt or nghi_quyet, it will use the existing template engine. For all other types (which are in Markdown), it will use your coworker's provided logic to convert Markdown to a DOCX file.

Step-by-Step Code Modifications
Step 1: Update the AI Intent Analysis Prompt

First, let's teach the AI to recognize the new summary types during chat.

File to Edit: app/services/ai_service.py

code
Python
download
content_copy
expand_less
# app/services/ai_service.py

# ...

INTENT_ANALYSIS_PROMPT = """Bạn là một mô hình AI chuyên phân tích ý định của người dùng trong một cuộc trò chuyện về biên bản họp. Phân tích câu cuối cùng của người dùng và trả về một đối tượng JSON DUY NHẤT.

Các loại 'intent' hợp lệ:
- 'edit_summary': Người dùng muốn chỉnh sửa, thay đổi, thêm, hoặc xóa nội dung của một bản tóm tắt.
- 'ask_question': Người dùng đang hỏi một câu hỏi về nội dung cuộc họp hoặc tóm tắt.
- 'general_chit_chat': Người dùng đang nói chuyện phiếm hoặc chào hỏi.

Các loại 'entity' (loại tóm tắt) hợp lệ:
- 'topic': Tóm tắt theo chủ đề chính.
- 'speaker': Tóm tắt theo người nói.
- 'action_items': Tóm tắt các công việc cần làm.
- 'decision_log': Tóm tắt các quyết định cuối cùng.
# --- ADD THESE NEW ENTITIES ---
- 'summary_bbh_hdqt': Biên bản họp dạng chi tiết (JSON).
- 'summary_nghi_quyet': Nghị quyết cuộc họp (JSON).
# --- END OF ADDITION ---
- null: Nếu không thể xác định hoặc người dùng không đề cập.

Cấu trúc JSON đầu ra BẮT BUỘC:
{
  "intent": "...",
  "entity": "...",
  "confidence": <số từ 0.0 đến 1.0>,
  "edit_instruction": "<Trích xuất chính xác chỉ thị chỉnh sửa của người dùng nếu có, nếu không thì để là null>"
}
# ... (rest of the prompt is unchanged)
Step 2: Refactor the Summary Generation Endpoint

This is the most critical change. We will transform the /summary endpoint so it becomes the single source for getting, generating, and saving all summary types. It will now check the database first before doing any work.

File to Edit: app/api/routes/meeting.py

code
Python
download
content_copy
expand_less
# app/api/routes/meeting.py

@router.post(
    "/{request_id}/summary",
    response_model=SummaryResponse,
    summary="Get, or generate and save, a meeting summary"
)
async def get_or_generate_summary( # <-- RENAME a little for clarity
    summary_request: SummaryRequest,
    db: Session = Depends(get_db_session),
    job: MeetingJob = Depends(get_owned_job_from_path)
):
    """
    Retrieves a summary if it already exists in the database.
    If not, it generates the summary, saves it permanently, and then returns it.
    This applies to ALL summary types, including 'summary_bbh_hdqt' and 'summary_nghi_quyet'.
    """
    db.add(job)
    summary_type = summary_request.summary_type
    logger.info(f"Request for summary '{summary_type}' for job '{job.request_id}'.")

    # --- 1. CHECK DATABASE FIRST ---
    existing_summary = db.exec(
        select(Summary).where(
            Summary.meeting_job_id == job.id,
            Summary.summary_type == summary_type
        )
    ).first()

    if existing_summary:
        logger.info(f"Found existing '{summary_type}' summary in DB. Returning cached version.")
        return SummaryResponse(
            request_id=job.request_id,
            summary_type=existing_summary.summary_type,
            summary_content=existing_summary.summary_content
        )

    # --- 2. IF NOT FOUND, GENERATE AND SAVE ---
    logger.info(f"No existing summary found. Generating a new '{summary_type}' summary.")
    
    # Prepare transcript source text (same as before)
    # ... [This block of code for getting `source_text` is unchanged] ...
    if summary_type == "speaker":
        if not job.diarized_transcript:
            raise HTTPException(status_code=400, detail="A 'speaker' summary requires diarization.")
        transcript_source = job.diarized_transcript.transcript_data
        source_text = "\n".join([f"{seg['speaker']}: {seg['text']}" for seg in transcript_source])
    else:
        transcription_entry = db.exec(select(Transcription).where(Transcription.meeting_job_id == job.id, Transcription.language == job.language)).first()
        if not transcription_entry or not transcription_entry.transcript_data:
            raise HTTPException(status_code=400, detail="A summary requires a completed transcript.")
        transcript_source = transcription_entry.transcript_data
        source_text = "\n".join([seg['text'] for seg in transcript_source])

    # Generate content using AI service
    try:
        meeting_info = { "bbh_name": job.bbh_name, "meeting_type": job.meeting_type, "meeting_host": job.meeting_host, }
        # For document generation, we need to add more context
        if summary_type in ["summary_bbh_hdqt", "summary_nghi_quyet"]:
             # This is the context preparation from your old download endpoint
            local_tz = ZoneInfo("Asia/Ho_Chi_Minh")
            start_time_local = job.upload_started_at.replace(tzinfo=timezone.utc).astimezone(local_tz) if job.upload_started_at else None
            end_time_local = job.upload_finished_at.replace(tzinfo=timezone.utc).astimezone(local_tz) if job.upload_finished_at else None
            context_header = (
                f"**THÔNG TIN BỐI CẢNH CUỘC HỌP:**\n"
                f"- Ngày họp: {start_time_local.strftime('%d/%m/%Y') if start_time_local else 'N/A'}\n"
                f"- Giờ bắt đầu: {start_time_local.strftime('%H:%M') if start_time_local else 'N/A'}\n"
                f"- Giờ kết thúc: {end_time_local.strftime('%H:%M') if end_time_local else 'N/A'}\n\n"
                f"**NỘI DUNG BIÊN BẢN (TRANSCRIPT):**\n"
            )
            source_text = context_header + source_text

        summary_content = await ai_service.get_response(
            task=summary_type,
            user_message=source_text,
            context={"meeting_info": meeting_info}
        )
    except Exception as e:
        logger.error(f"AI service failed during summary generation: {e}", exc_info=True)
        raise HTTPException(status_code=502, detail=f"Failed to get response from AI service: {e}")

    # --- 3. SAVE THE NEWLY GENERATED SUMMARY TO DB ---
    new_summary = Summary(
        meeting_job_id=job.id,
        summary_type=summary_type,
        summary_content=summary_content
    )
    db.add(new_summary)
    db.commit()
    db.refresh(new_summary)

    logger.info(f"Successfully generated and saved new '{summary_type}' summary.")
    return SummaryResponse(
        request_id=job.request_id,
        summary_type=new_summary.summary_type,
        summary_content=new_summary.summary_content
    )
Step 3: Integrate Coworker's Code and Refactor Download Endpoint

First, let's add your coworker's logic into your project. We'll place it in the document generation service file.

File to Edit: app/services/document_generator.py

code
Python
download
content_copy
expand_less
# app/services/document_generator.py

import json
import logging
from io import BytesIO
from pathlib import Path
from docxtpl import DocxTemplate
from app.utils import create_subdoc_from_structured_data
import pypandoc # <-- ADD THIS IMPORT
import os # <-- ADD THIS IMPORT

logger = logging.getLogger(__name__)
# ...

# --- THIS IS THE ORIGINAL FUNCTION FOR TEMPLATES ---
def generate_templated_document(template_type: str, llm_json_output: str) -> BytesIO:
    # ... (this function is unchanged)

# --- ADD THIS NEW FUNCTION BASED ON YOUR COWORKER'S CODE ---
def generate_docx_from_markdown(markdown_content: str) -> BytesIO:
    """
    Converts a markdown string to a DOCX file in memory using Pandoc
    and returns it as a BytesIO buffer.
    """
    # Use a temporary directory to avoid file conflicts
    temp_dir = Path("./temp_docs")
    os.makedirs(temp_dir, exist_ok=True)
    
    markdown_file_path = temp_dir / 'temp.md'
    docx_output_path = temp_dir / 'temp_output.docx'

    try:
        with open(markdown_file_path, 'w', encoding='utf-8') as f:
            f.write(markdown_content)

        logger.info(f"Converting markdown to DOCX using Pandoc...")
        pypandoc.convert_file(
            str(markdown_file_path),
            to='docx',
            outputfile=str(docx_output_path),
            extra_args=['--standalone']
        )
        logger.info("Pandoc conversion successful!")

        with open(docx_output_path, "rb") as f:
            buffer = BytesIO(f.read())
        buffer.seek(0)
        return buffer

    except Exception as e:
        logger.error(f"Pandoc conversion failed: {e}. Falling back to basic conversion.", exc_info=True)
        # You can add a simpler fallback here if needed, or just re-raise
        raise RuntimeError(f"Failed to generate DOCX from markdown: {e}")
    finally:
        # Clean up temporary files
        if os.path.exists(markdown_file_path):
            os.remove(markdown_file_path)
        if os.path.exists(docx_output_path):
            os.remove(docx_output_path)

Now, let's replace your old download endpoint with the new, unified one.

File to Edit: app/api/routes/meeting.py

code
Python
download
content_copy
expand_less
# app/api/routes/meeting.py

# --- Make sure this is imported ---
from app.services.document_generator import generate_templated_document, generate_docx_from_markdown

# ...

# --- REPLACE THE OLD 'generate_and_download_document' WITH THIS ---
@router.get("/{request_id}/download/document", summary="Download any summary as a DOCX document")
async def download_summary_document(
    job: MeetingJob = Depends(get_owned_job_from_path),
    summary_type: str = Query(..., description="The type of summary to download."),
    db: Session = Depends(get_db_session)
):
    """
    Downloads the latest version of any given summary type as a .docx file.
    It fetches the content from the database. If the summary hasn't been generated yet,
    it will return an error.
    """
    # 1. Fetch the required summary from the database
    summary = db.exec(
        select(Summary).where(
            Summary.meeting_job_id == job.id,
            Summary.summary_type == summary_type
        )
    ).first()

    if not summary:
        raise HTTPException(
            status_code=status.HTTP_404_NOT_FOUND,
            detail=f"Summary of type '{summary_type}' has not been generated yet. Please generate it first."
        )

    # 2. Select the correct document generation engine
    try:
        if summary_type in ["summary_bbh_hdqt", "summary_nghi_quyet"]:
            # These types use the template engine with JSON data
            document_buffer = generate_templated_document(summary_type.replace("summary_", ""), summary.summary_content)
        else:
            # All other types are Markdown and use the Pandoc engine
            document_buffer = generate_docx_from_markdown(summary.summary_content)
    except Exception as e:
        logger.error(f"Error during document generation for type '{summary_type}': {e}", exc_info=True)
        raise HTTPException(status_code=500, detail=f"Failed to generate the document: {e}")

    # 3. Stream the response
    filename = f"{summary_type}_{job.bbh_name.replace(' ', '_')}.docx"
    headers = {"Content-Disposition": f"attachment; filename*=UTF-8''{filename}"}
    
    return StreamingResponse(
        document_buffer,
        media_type="application/vnd.openxmlformats-officedocument.wordprocessingml.document",
        headers=headers
    )

Your application is now fully upgraded. All summary types are treated as first-class citizens: they are persistently stored, editable via chat, and downloadable in the correct format. The logic is centralized, robust, and provides the intelligent, conversational experience you described.
