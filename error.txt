x-common-variables: &common-environment
  DATABASE_URL: "postgresql+psycopg2://${POSTGRES_USER}:${POSTGRES_PASSWORD}@10.43.128.107:5432/${POSTGRES_DB}"
  REDIS_URL: "redis://:${REDIS_PASSWORD}@10.43.128.107:6379/0"
  AZURE_OPENAI_API_KEY: "${AZURE_OPENAI_API_KEY}"
  AZURE_OPENAI_ENDPOINT: "${AZURE_OPENAI_ENDPOINT}"
  AZURE_OPENAI_API_VERSION: "${AZURE_OPENAI_API_VERSION}"
  AZURE_OPENAI_DEPLOYMNENT_NAME: "gpt-4.1-GS"
  LITE_LLM_API_KEY: "sk-ViUwPBWOrdINDxfldb69Fw"
  LITE_LLM_BASE_URL: "https://ai-platform.vietinbank.vn/"
  LITE_LLM_MODEL_NAME: "gpt-4.1-GS"
  QDRANT_HOST: "10.43.128.107"
  QDRANT_PORT: "6333"
  QDRANT_COLLECTION_NAME: "meeting_speakers"
  CUDNN_H_PATH: "/usr/include/cudnn.h"
  RIMECASTER_MODEL_PATH: "/app/models/rimecaster.nemo"
  FASTER_WHISPER_MODEL_PATH: "/app/models/merged_model_ct2_dir"


services:
  # -----------------------------------------------------------------
  #  1. API Service (The Web Server)
  #  - Handles incoming HTTP requests quickly.
  #  - Offloads heavy work to the worker.
  #  - Does NOT use the GPU.
  # -----------------------------------------------------------------
  api:
    build:
      context: .
      dockerfile: Dockerfile
    container_name: diarization_pipeline_server
    command: "uvicorn main:app --host 0.0.0.0 --port 8020 --reload"
    ports:
      - "8020:8020" 
    volumes:
      - ./app:/app
      #- /usr/include:/usr/include
      - /usr/local/cuda/lib64:/usr/local/cuda/lib64
      - ./shared_audio:/app/shared_audio
      #- /usr/lib/x86-64-linux-gnu/libcudnn_cnn.so.9:/usr/lib/x86-64-linux-gnu/libcudnn_cnn.so.9
    environment:
      <<: *common-environment
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1 
              capabilities: [gpu]
    restart: unless-stopped

  # -----------------------------------------------------------------
  #  2. Worker Service (The Heavy Lifter)
  #  - Consumes tasks from Redis (Celery).
  #  - Has dedicated GPU access for AI/ML models.
  #  - Shares the same codebase as the API.
  # -----------------------------------------------------------------
  
  worker-transcription:
    build:
      context: .
      dockerfile: Dockerfile
    container_name: pipeline_transcription_worker
    command: "celery -A processing.tasks.celery_app worker -l info --pool=solo -Q gpu_transcription --concurrency=1"
    volumes:
      - ./app:/app
      - ./models:/app/models
      - /usr/local/cuda/lib64:/usr/local/cuda/lib64
      - ./shared_audio:/app/shared_audio
    environment:
      <<: *common-environment
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1 
              capabilities: [gpu]
    restart: unless-stopped


  worker-diarization:
    build:
      context: .
      dockerfile: Dockerfile
    container_name: pipeline_diarization_worker
    command: "celery -A processing.tasks.celery_app worker -l info --pool=solo -Q gpu_diarization --concurrency=1"
    volumes:
      - ./app:/app
      - ./models:/app/models
      - /usr/local/cuda/lib64:/usr/local/cuda/lib64
      - ./shared_audio:/app/shared_audio
    environment:
      <<: *common-environment
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1 
              capabilities: [gpu]
    restart: unless-stopped


  worker-cpu:
    build:
      context: .
      dockerfile: Dockerfile
    container_name: pipeline_cpu_worker
    command: "celery -A processing.tasks.celery_app worker -l info -Q cpu_tasks --concurrency=2"
    volumes:
      - ./app:/app
      - ./models:/app/models
      - ./shared_audio:/app/shared_audio
    environment:
      <<: *common-environment
    restart: unless-stopped

    

#   worker:
#     build:
#       context: .
#       dockerfile: Dockerfile
#     container_name: diarization_full_worker
#     command: "celery -A processing.tasks.celery_app worker -l info --pool=solo --concurrency=1"
#     #command: "sleep infinity"
#     volumes:
#       - ./app:/app
#       #- /usr/include:/usr/include
#       - ./models:/app/models
#       - /usr/local/cuda/lib64:/usr/local/cuda/lib64
#       - shared_audio:/app/shared_audio
#       #- /usr/lib/x86-64-linux-gnu/libcudnn_cnn.so.9:/usr/lib/x86-64-linux-gnu/libcudnn_cnn.so.9
#     environment:
#       - DATABASE_URL=postgresql+psycopg2://${POSTGRES_USER}:${POSTGRES_PASSWORD}@10.43.128.107:5432/${POSTGRES_DB}
#       - REDIS_URL=redis://:${REDIS_PASSWORD}@10.43.128.107:6379/0
#       - AZURE_OPENAI_API_KEY=${AZURE_OPENAI_API_KEY}
#       - AZURE_OPENAI_ENDPOINT=${AZURE_OPENAI_ENDPOINT}
#       - AZURE_OPENAI_API_VERSION=${AZURE_OPENAI_API_VERSION}
#       - QDRANT_HOST=10.43.128.107
#       - QDRANT_PORT=6333
#       - QDRANT_COLLECTION_NAME=meeting_speakers
#       - CUDNN_H_PATH=/usr/include/cudnn.h
#       - RIMECASTER_MODEL_PATH=/app/models/rimecaster.nemo
#       - FASTER_WHISPER_MODEL_PATH=/app/models/merged_model_ct2_dir
#     depends_on:
#       - api 

#     deploy:
#       resources:
#         reservations:
#           devices:
#             - driver: nvidia
#               count: 1 
#               capabilities: [gpu]
#     restart: unless-stopped

volumes:
  shared_audio:
    driver: local
    
