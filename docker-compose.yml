services:

  api:
    build:
      context: .
      dockerfile: Dockerfile
    container_name: meeting_api_server
    command: "uvicorn app.main:app --host 0.0.0.0 --port 8072 --reload"
    ports:
      - "8072:8072"
    volumes:
      - ./app:/code/app
      - ./shared_audio:/code/shared_audio
    env_file: .env
    restart: unless-stopped

  # -------------------------------------------------------------
  #  GPU Worker: Executes heavy AI tasks
  # -------------------------------------------------------------
  worker:
    build:
      context: .
      dockerfile: Dockerfile
    container_name: meeting_worker
    command: >
      celery -A app.worker.celery_app worker -l info 
      --pool=solo 
      --concurrency=1
    # environment:
    #   - NVIDIA_VISIBLE_DEVICES=a11
    #   - NVIDIA_DRIVER_CAPABILITIES=compute,utility
    #   - LD_LIBRARY_PATH=/usr/local/cuda/lib64:/usr/lib/x86_64-linux-gnu
    volumes:
      - ./app:/code/app
      - ./shared_audio:/code/shared_audio
      # - /usr/lib/x86_64-linux-gnu:/usr/lib/x86_64-linux-gnu
      # - /usr/local/cuda-12.8/targets/x86_64-linux/lib:/usr/local/cuda/lib64
      - /usr/local/cuda/lib64:/usr/local/cuda/lib64

    env_file: .env
    restart: unless-stopped
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]

  # -------------------------------------------------------------
  #  CPU Worker
  # -------------------------------------------------------------
  # worker-cpu:
  #   build:
  #     context: .
  #     dockerfile: Dockerfile
  #   container_name: meeting_cpu_worker
  #   command: >
  #     celery -A app.worker.celery_app worker -l info 
  #     -Q cpu_tasks 
  #     --concurrency=4
  #   volumes:
  #     - ./app:/code/app
  #     - ./shared_audio:/code/shared_audio
  #   env_file: .env
  #   restart: unless-stopped

